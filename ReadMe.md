#Deep Learning Models Playground
This repository contains implementations of foundational deep learning architectures, ranging from simple feedforward networks to modern transformer-based models.
Itâ€™s designed as a learning resource and reference for understanding how these models work, how theyâ€™re trained, and how they differ from each other.

ðŸ“š Implemented Models
ðŸ”¹ 1. MLP / Feedforward Neural Network
Basic fully connected layers with activation functions.
Serves as a foundation for all subsequent deep learning models.
Example use case: MNIST digit classification.

ðŸ”¹ 2. Convolutional Neural Networks (CNNs)
Includes both a basic CNN and the classic LeNet-5 architecture.
Designed for image classification tasks by capturing spatial hierarchies in data.
Example use case: MNIST / CIFAR-10.

ðŸ”¹ 3. Recurrent Neural Network (RNN)
Implements a vanilla RNN for sequence modeling.
Learns dependencies in sequential data.
Example use case: character-level text generation.

ðŸ”¹ 4. Long Short-Term Memory (LSTM)
Extension of RNNs designed to handle long-term dependencies.
Uses gating mechanisms to overcome vanishing gradients.
Example use case: sequence classification (e.g., sentiment analysis).

ðŸ”¹ 5. Gated Recurrent Unit (GRU)
A simplified variant of LSTM with fewer parameters.
More efficient while retaining the ability to capture long-term dependencies.
Example use case: time-series forecasting.

ðŸ”¹ 6. Transformer (BERT Intro)
Transformer architecture with self-attention mechanism.
Includes a simplified introduction to BERT (Bidirectional Encoder Representations from Transformers).
Example use case: sentence classification.
